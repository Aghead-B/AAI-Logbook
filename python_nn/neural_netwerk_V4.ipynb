{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht: Build your own neural net from scratch.\n",
    "### Auteur: Aghead Bilal\n",
    "### Datum: 2023-01-03\n",
    "### Versie: 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeren van de benodigde libraries\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het doel is het implementeren van een complete neuraal net met alleen dense layers dat te trainen is voor een bepaalde taak.\n",
    "De taak die we gaan gebruiken is Maak en test netwerk en test hem b.v op de xor functie. \n",
    "Stappenplan.\n",
    "\n",
    "- 1: Schrijf de activatie functies als Relu en Sigmoid dusdanig dat als aangeroepen met een numpy array ze een array van dezelfde vorm teruggeven waarin de functie op elk element is toegepast. Maak ook de afgeleide fucnties. Implementeer ook de Softmax functie voor de outputlaag. <br>\n",
    "\n",
    "- 2: Verzin een datastructuur (b.v python list) waarin je een laag van een NN kan opslaan, Denk aan minimaal weights, biases en activatie functies. Maak ook de functies om een laag te initialiseren. <br>\n",
    "\n",
    "- 3: Schrijf de feedforward functie die de input neemt en een geheel net doorrekend. <br>\n",
    "\n",
    "- 4: Bepaal de output error bij een gegeven in- en output en back-propagate door het netwerk zodat je van iedere laag de error kent.  <br>\n",
    "\n",
    "- 5: Update de weights en biases gebaseerd op de errors in de laag en de afgeleide van de gebruikte errorfunctie en activatiefunctie. <br>\n",
    "\n",
    "- 6: Schrijf een functie die een netwerk kan trainen op een gegeven dataset. <br>\n",
    "\n",
    "- 7: Schrijf een functie die een netwerk kan testen op een gegeven dataset. <br>\n",
    "\n",
    "- 8: Maak een netwerk dat de XOR functie kan leren. <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Schrijf de activatie functies als Relu en Sigmoid dusdanig dat als aangeroepen met een numpy array ze een array van dezelfde vorm teruggeven waarin de functie op elk element is toegepast. Maak ook de afgeleide fucnties. Implementeer ook de Softmax functie voor de outputlaag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu functie\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Relu afgeleide functie\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "# Sigmoid functie\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid afgeleide functie\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Softmax functie\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Verzin een datastructuur (b.v python list) waarin je een laag van een NN kan opslaan, Denk aan minimaal weights, biases en activatie functies. Maak ook de functies om een laag te initialiseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datastructuur\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, activation=None, weights=None, biases=None):\n",
    "        self.weights = weights if weights is not None else 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = biases if biases is not None else np.zeros((1, n_neurons))\n",
    "        self.activation = activation\n",
    "        self.output = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        if self.activation is not None:\n",
    "            self.output = self.activation(self.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Schrijf de feedforward functie die de input neemt en een geheel net doorrekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward functie\n",
    "def feedforward(X, network):\n",
    "    layer_outputs = []\n",
    "    for layer in network:\n",
    "        layer.forward(X)\n",
    "        layer_outputs.append(layer.output)\n",
    "        X = layer.output\n",
    "    return layer_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Bepaal de output error bij een gegeven in- en output en back-propagate door het netwerk zodat je van iedere laag de error kent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation functie\n",
    "def backpropagation(y, network, layer_outputs):\n",
    "    errors = []\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        if layer == network[-1]:\n",
    "            errors.insert(0, y - layer_outputs[i])\n",
    "        else:\n",
    "            errors.insert(0, np.dot(errors[0], network[i + 1].weights.T))\n",
    "        layer.error = errors[0] * relu_derivative(layer_outputs[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Update de weights en biases gebaseerd op de errors in de laag en de afgeleide van de gebruikte errorfunctie en activatiefunctie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update functie\n",
    "def update(network, X, learning_rate):\n",
    "    for i in range(len(network)):\n",
    "        layer = network[i]\n",
    "        if i == 0:\n",
    "            inputs = X\n",
    "        else:\n",
    "            inputs = network[i - 1].output\n",
    "        layer.weights += learning_rate * np.dot(inputs.T, layer.error)\n",
    "        layer.biases += learning_rate * np.sum(layer.error, axis=0, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Schrijf een functie die een netwerk kan trainen op een gegeven dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train functie\n",
    "def train(X, y, network, learning_rate, n_iterations):\n",
    "    for i in range(n_iterations):\n",
    "        layer_outputs = feedforward(X, network)\n",
    "        backpropagation(y, network, layer_outputs)\n",
    "        update(network, X, learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Schrijf een functie die een netwerk kan testen op een gegeven dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test functie\n",
    "def test(X, y, network):\n",
    "    layer_outputs = feedforward(X, network)\n",
    "    predictions = np.argmax(layer_outputs[-1], axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Maak een netwerk dat de XOR functie kan leren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "# XOR data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Maak netwerk\n",
    "network = []\n",
    "network.append(Layer_Dense(2, 4, activation=relu))\n",
    "network.append(Layer_Dense(4, 4, activation=relu))\n",
    "network.append(Layer_Dense(4, 1, activation=sigmoid))\n",
    "\n",
    "# Train netwerk\n",
    "train(X, y, network, learning_rate=0.3, n_iterations=100000)\n",
    "\n",
    "# Test netwerk\n",
    "accuracy = test(X, y, network)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
