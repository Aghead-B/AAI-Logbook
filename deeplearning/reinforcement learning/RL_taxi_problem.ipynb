{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\").env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "streets.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the open source library — gym and start working with the taxi environment. Here we used version 3 but new versions keeps getting released, so if it doesn’t work then try v2 or v4.\n",
    "\n",
    "We can use streets.render() to output a visualization of our environment, with our cute little taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "initial_state = streets.encode(2, 3, 2, 0) # 1,0,2,0\n",
    "\n",
    "streets.s = initial_state\n",
    "\n",
    "streets.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a initial state for our problem, the agent start at row index 2 and column index 3, we need to pick up customer from Yellow spot and drop him off at Red spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "# a 2D array that represent every possible state and action in the virtual space and initialize all of them to 0\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.6\n",
    "exploration = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample() # Explore a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "            \n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        \n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        q_table[state, action] = new_q\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A initialization of the q table is generated as a 2D Numpy array that represents every possible state and action pair in our virtual space. The parameters are specified , 10000 epochs means we would repeatedly let the taxi explore for 10000 times.\n",
    "\n",
    "The epsilon greedy strategy comes into play when we generate a random value from the uniform distribution between 0 and 1 and compare it to our epsilon ( exploration rate ), if the random value s smaller we take a random action from our action space and if not we look at our current Q-table and take the action that maximize the value function.\n",
    "\n",
    "We then perform the action, observe the reward and update our Q value and update our state(st) to be the “next state”(st+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.40301019, -2.39145865, -2.4122798 , -2.3639511 , -5.81019962,\n",
       "       -7.67740217])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[initial_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.12208712, -2.21494995, -2.22015584, -2.21129333, -4.47957617,\n",
       "       -7.05140615])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[streets.encode(1,0,2,0)]  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the result! If we’re at the initial state, the highest value is -2.3639511, which corresponds to taking a step to the left, and if we look back to our original visualization, we can see that if the taxi wants to pick up the passenger from Yellow spot, moving to the left is obviously the best choice! You can play with this and check for different states, it is a great sanity check to see if your model converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 10 Step 11\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "lengths=[]\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()\n",
    "   \n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info = streets.step(action)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "        print(streets.render(mode='ansi'))\n",
    "        sleep(.2)\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "    lengths.append(trip_length)\n",
    "    \n",
    "    sleep(.2)\n",
    "avg_len=sum(lengths)/10\n",
    "print(avg_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Q-table we have computed we could instruct the taxi driver to do his job now. This piece of code randomly initialize a state and ask the taxi driver to perform the task according to the Q-table generated. We’ve restricted the number of steps the taxi could take to a maximum of 25 steps in case we have a taxi driver that has really bad luck and we repeat this for 10 times, calculate the average number of steps it takes for the taxi driver to complete the task to evaluate our model.\n",
    "\n",
    "We can see that the average number of step is 12.1, not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(learning_rate,discount_factor,exploration,epochs):\n",
    "    q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "# a 2D array that represent every possible state and action in the virtual space and initialize all of them to 0\n",
    "    for taxi_run in range(epochs):\n",
    "        state = streets.reset()\n",
    "        done = False\n",
    "            \n",
    "        while not done:\n",
    "            random_value = random.uniform(0, 1)\n",
    "            if (random_value < exploration):\n",
    "                action = streets.action_space.sample() # Explore a random action\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Use the action with the highest q-value\n",
    "                    \n",
    "            next_state, reward, done, info = streets.step(action)\n",
    "                \n",
    "            prev_q = q_table[state, action]\n",
    "            next_max_q = np.max(q_table[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table[state, action] = new_q\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "\n",
    "\n",
    "def average_trip_length():\n",
    "    lengths=[]\n",
    "    for tripnum in range(1, 11):\n",
    "        state = streets.reset()\n",
    "        done = False\n",
    "        trip_length = 0\n",
    "        \n",
    "        while not done and trip_length < 25:\n",
    "            action = np.argmax(q_table[state])\n",
    "            next_state, reward, done, info = streets.step(action)\n",
    "            clear_output(wait=True)\n",
    "            state = next_state\n",
    "            trip_length += 1\n",
    "        lengths.append(trip_length)\n",
    "    avg_len=sum(lengths)/10\n",
    "    return avg_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve upon our existing model, we need to tune our hyperparameters. This step is extremely important if we want our model to achieve better result, in this case, we want the taxi driver to be able to carry out the task with less step. So first I’ve re-defined the Q- learning process and calculation of average trip length functions to test different learning rate, discount factor and exploration rates (epsilon).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.09 12.76 13.19 12.49 12.  ]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "discount_factor = [0.5,0.6,0.7,0.8,0.9]\n",
    "exploration = 0.1\n",
    "epochs = 1000\n",
    "difdis=[0,0,0,0,0]\n",
    "for j in range(1,10):\n",
    "    for i in range(len(discount_factor)):\n",
    "        q_learning(learning_rate,discount_factor[i],exploration,epochs)\n",
    "        difdis[i]+=average_trip_length()\n",
    "\n",
    "print(np.array(difdis)/10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that because we the initial states are randomly generated when we test for average number of steps, each time the result could be different, so we take the average of 10 trails for each average of 10 trails! Sounds confusing, but if you look closely at the code above it would become clearer. The best discount factor turns out to be `0.9`, so we fix 0.9 and try different learning rates, and do the same for exploration rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.3  12.26 12.3  12.64 12.89]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.1,0.2,0.3,0.4,0.5]\n",
    "discount_factor = 0.9\n",
    "exploration = 0.1\n",
    "epochs = 1000\n",
    "difdis=[0,0,0,0,0]\n",
    "for j in range(1,10):\n",
    "    for i in range(len(learning_rate)):\n",
    "        q_learning(learning_rate[i],discount_factor,exploration,epochs) \n",
    "        difdis[i]+=average_trip_length()\n",
    "\n",
    "print(np.array(difdis)/10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it appears that the average trip length is the shortest or most optimal when the learning rate is set to `0.1` or 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.99 12.38 11.91 12.36]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration = [0.1,0.2,0.3,0.4]\n",
    "epochs = 1000\n",
    "difdis=[0,0,0,0]\n",
    "for j in range(1,10):\n",
    "    for i in range(len(exploration)):\n",
    "        q_learning(learning_rate,discount_factor,exploration[i],epochs)\n",
    "        difdis[i]+=average_trip_length()\n",
    "\n",
    "print(np.array(difdis)/10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it appears that the average trip length is the shortest or most optimal when the exploration is set to `0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.82\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration = 0.3\n",
    "epochs = 1000\n",
    "difdis=[]\n",
    "for j in range(1,10):\n",
    "        q_learning(learning_rate,discount_factor,exploration,epochs)\n",
    "        difdis.append(average_trip_length())\n",
    "\n",
    "print(sum(difdis)/10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, combining the optimal outcomes for each parameter, we’ve achieved a much better result, the average number of steps reduced from 21.1 to 11.8!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
